{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6b20376-9f1a-4ce1-9a8f-8be5323b2dba",
   "metadata": {},
   "source": [
    "mejorar los resultados del modelo de Latent Dirichlet Allocation (LDA) y de procesamiento de textos en general. Aquí hay algunas sugerencias que podrías considerar:\n",
    "1. Preprocesamiento de Textos\n",
    "\n",
    "Mejora el preprocesamiento de los textos para obtener una mejor calidad de los temas:\n",
    "\n",
    "    Normalización: Asegúrate de que la normalización del texto esté completa. Por ejemplo, verifica que se estén manejando bien las tildes y caracteres especiales.\n",
    "    Lematización en lugar de Stemming: La lematización puede ser más efectiva que el stemming en ciertos casos, ya que la lematización reduce las palabras a su forma base real en lugar de solo eliminar sufijos. Aunque en tu caso, estás usando un stemmer y no lematización, podrías considerar lematizar.\n",
    "    Manejo de N-gramas: Considera usar n-gramas (por ejemplo, bigramas o trigramas) para capturar contextos más amplios de las palabras.\n",
    "\n",
    "Aquí está un código actualizado para lematización y n-gramas:\n",
    "\n",
    "python\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.util import ngrams\n",
    "\n",
    "# Inicializar el lematizador\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess(text):\n",
    "    # Normalizar el texto\n",
    "    text = text.lower()\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('ascii')\n",
    "    \n",
    "    # Eliminar caracteres no alfabéticos y puntuación\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Tokenizar el texto\n",
    "    tokens = word_tokenize(text, language='spanish')\n",
    "    \n",
    "    # Lematizar y eliminar stopwords\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words and len(word) > 3]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "2. Ajuste de Parámetros del Modelo\n",
    "\n",
    "Ajusta los parámetros del modelo LDA:\n",
    "\n",
    "    Número de Componentes: Prueba diferentes valores para n_components para ver si obtienes temas más coherentes.\n",
    "    Parámetros de Regularización: Ajusta alpha y beta para ver cómo afectan la calidad de los temas. Puedes usar learning_method='online' para actualizaciones más frecuentes del modelo.\n",
    "\n",
    "Ejemplo de ajuste:\n",
    "\n",
    "python\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=5, max_iter=10, learning_method='online', random_state=42, alpha=0.1, beta=0.01)\n",
    "lda.fit(X)\n",
    "\n",
    "3. Evaluación de la Coherencia de los Temas\n",
    "\n",
    "Puedes evaluar la coherencia de los temas utilizando métricas de coherencia. Una manera común es usar el Coherence Score.\n",
    "\n",
    "Aquí hay un ejemplo usando gensim para calcular la coherencia:\n",
    "\n",
    "python\n",
    "\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "\n",
    "# Crear el diccionario y corpus para Gensim\n",
    "texts_gensim = [text.split() for text in joined_texts.values()]  # Asegúrate de que el texto esté tokenizado\n",
    "dictionary = Dictionary(texts_gensim)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts_gensim]\n",
    "\n",
    "# Crear el modelo LDA de Gensim\n",
    "lda_gensim = LdaModel(corpus, num_topics=5, id2word=dictionary, passes=15)\n",
    "\n",
    "# Calcular la coherencia\n",
    "coherence_model_lda = CoherenceModel(model=lda_gensim, texts=texts_gensim, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print(f'Coherence Score: {coherence_lda}')\n",
    "\n",
    "4. Revisar y Ajustar el Código para TF-IDF\n",
    "\n",
    "Asegúrate de que el modelo de TF-IDF esté bien configurado y de que las stopwords y palabras cortas sean adecuadamente manejadas. Verifica también si el parámetro token_pattern está excluyendo las palabras deseadas.\n",
    "Resumen\n",
    "\n",
    "La clave es ajustar y probar diferentes configuraciones y técnicas para ver cuáles producen los resultados más coherentes y útiles para tu análisis. ¡Espero que estas sugerencias te ayuden a mejorar los resultados de tus temas!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768b1cc9-503d-4ff9-96cf-e5e05d129145",
   "metadata": {},
   "source": [
    "código de visualización de gráficos basado en las palabras más frecuentes de los textos de un autor, debes asegurarte de que word_frequencies contenga los datos correctos de las frecuencias de palabras, y luego puedes utilizar matplotlib para generar el gráfico. A continuación te explico cómo actualizarlo:\n",
    "Pasos a seguir:\n",
    "\n",
    "    Asegúrate de que word_frequencies esté definido:\n",
    "        word_frequencies debe ser un diccionario donde las claves sean los nombres de los autores ('keynes' y 'hayek'), y los valores sean las frecuencias de palabras para cada autor. Este diccionario debe haber sido calculado previamente en el análisis.\n",
    "\n",
    "    Modificar la selección del autor:\n",
    "        En lugar de seleccionar el autor directamente en el código, podrías hacerlo dinámico, permitiendo visualizar las palabras más frecuentes de cualquier autor.\n",
    "\n",
    "    Actualizar el gráfico:\n",
    "        Puedes mejorar la visualización con etiquetas más legibles y un formato general más claro.\n",
    "\n",
    "Código actualizado:\n",
    "\n",
    "python\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Función para graficar las palabras más frecuentes de un autor\n",
    "def plot_top_words(author, word_frequencies, top_n=10):\n",
    "    # Obtener las palabras más comunes\n",
    "    most_common_words = word_frequencies[author].most_common(top_n)\n",
    "    words, counts = zip(*most_common_words)\n",
    "\n",
    "    # Crear el gráfico de barras\n",
    "    plt.figure(figsize=(10, 6))  # Ajustar el tamaño del gráfico\n",
    "    plt.bar(words, counts, color='skyblue')\n",
    "    plt.title(f\"Top {top_n} words in {author.capitalize()}'s texts\", fontsize=16)\n",
    "    plt.xlabel(\"Words\", fontsize=14)\n",
    "    plt.ylabel(\"Frequencies\", fontsize=14)\n",
    "    plt.xticks(rotation=45, ha='right', fontsize=12)\n",
    "    plt.tight_layout()  # Ajustar el diseño para que no se superpongan elementos\n",
    "    plt.show()\n",
    "\n",
    "# Ejemplo de uso\n",
    "plot_top_words('keynes', word_frequencies)\n",
    "\n",
    "Detalles importantes:\n",
    "\n",
    "    author.capitalize(): Asegura que el nombre del autor en el título del gráfico se muestre con la primera letra en mayúscula.\n",
    "    plt.tight_layout(): Ajusta automáticamente el diseño del gráfico para que los elementos no se superpongan.\n",
    "    plt.figure(figsize=(10, 6)): Ajusta el tamaño del gráfico para que sea más legible.\n",
    "\n",
    "Notas adicionales:\n",
    "\n",
    "    Si tienes varios autores y deseas graficar las palabras más frecuentes de todos ellos, puedes iterar sobre los nombres de los autores y generar un gráfico para cada uno.\n",
    "\n",
    "Ejemplo para múltiples autores:\n",
    "\n",
    "python\n",
    "\n",
    "for author in word_frequencies:\n",
    "    plot_top_words(author, word_frequencies)\n",
    "\n",
    "Esto generará un gráfico separado para cada autor, mostrando las palabras más frecuentes según los datos de word_frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55bd424-858c-4fb1-ae18-e492bcdfabbb",
   "metadata": {},
   "source": [
    "Latent Dirichlet Allocation (LDA), un algoritmo de modelado de temas, para identificar grupos de palabras que tienden a aparecer juntas en los textos de cada autor. A continuación te explico cómo funciona y cómo debes modificarlo para ajustarlo a tu caso:\n",
    "1. Vectorización de los textos\n",
    "\n",
    "python\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='spanish')\n",
    "X = vectorizer.fit_transform(joined_texts.values())\n",
    "\n",
    "    max_df=0.95: Ignora las palabras que aparecen en más del 95% de los documentos, asumiendo que son demasiado comunes para ser informativas.\n",
    "    min_df=2: Ignora las palabras que aparecen en menos de 2 documentos, asumiendo que son demasiado raras para ser útiles.\n",
    "    stop_words='spanish': Elimina palabras vacías comunes en español.\n",
    "    fit_transform(joined_texts.values()): Convierte los textos en una matriz de características TF-IDF.\n",
    "\n",
    "2. Aplicar LDA\n",
    "\n",
    "python\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "lda.fit(X)\n",
    "\n",
    "    n_components=5: Indica que el modelo LDA buscará 5 temas distintos en los textos. Puedes ajustar este número según cuántos temas esperas encontrar.\n",
    "    random_state=42: Fija la semilla aleatoria para asegurar que los resultados sean reproducibles.\n",
    "\n",
    "3. Mostrar los temas\n",
    "\n",
    "python\n",
    "\n",
    "for idx, topic in enumerate(lda.components_):\n",
    "    print(f\"Topic #{idx}:\")\n",
    "    print([terms[i] for i in topic.argsort()[-10:]])\n",
    "\n",
    "    lda.components_: Contiene las relaciones entre los términos y los temas. Cada fila corresponde a un tema y cada columna a un término.\n",
    "    topic.argsort()[-10:]: Ordena los términos dentro de un tema por relevancia y selecciona los 10 más importantes.\n",
    "\n",
    "Modificaciones sugeridas:\n",
    "\n",
    "    Asegurarse de que el nombre terms esté definido correctamente:\n",
    "        terms = vectorizer.get_feature_names_out(): Asegura que terms se refiere a los nombres de los términos extraídos de TfidfVectorizer.\n",
    "    Corrección de codificación (si no se ha hecho ya):\n",
    "        Si has tenido problemas de codificación, asegúrate de que los textos se hayan preprocesado correctamente antes de aplicar este código.\n",
    "\n",
    "Código actualizado:\n",
    "\n",
    "python\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Unir todos los textos de un autor en un solo documento\n",
    "joined_texts = {author: ' '.join(texts[author]) for author in texts}\n",
    "\n",
    "# Vectorizar los textos\n",
    "vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='spanish')\n",
    "X = vectorizer.fit_transform(joined_texts.values())\n",
    "\n",
    "# Obtener términos\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Aplicar LDA\n",
    "lda = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "lda.fit(X)\n",
    "\n",
    "# Mostrar los temas\n",
    "for idx, topic in enumerate(lda.components_):\n",
    "    print(f\"Topic #{idx}:\")\n",
    "    print([terms[i] for i in topic.argsort()[-10:]])\n",
    "\n",
    "Notas:\n",
    "\n",
    "    Número de temas (n_components): Puedes ajustar n_components=5 a otro valor si consideras que es más adecuado para el análisis.\n",
    "    Filtrado (max_df y min_df): Puedes modificar max_df y min_df para ajustar la granularidad de los términos analizados, dependiendo de la cantidad de datos y la naturaleza del contenido."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
